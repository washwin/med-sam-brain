{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/washwin/med-sam-brain/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKc6-6LUMvaT"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "from torch.utils import tensorboard\n",
        "from tensorboardX import SummaryWriter\n",
        "#from dataset import *\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cfg\n",
        "import function\n",
        "from conf import settings\n",
        "#from models.discriminatorlayer import discriminator\n",
        "from dataset import *\n",
        "from utils import *\n",
        "from models.common import loralib as lora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1F813acMvaX"
      },
      "outputs": [],
      "source": [
        "\n",
        "args = cfg.parse_args()\n",
        "\n",
        "GPUdevice = torch.device('cuda', args.gpu_device)\n",
        "net = get_network(args, args.net, use_gpu=args.gpu, gpu_device=GPUdevice, distribution = args.distributed)\n",
        "if args.pretrain:\n",
        "    weights = torch.load(args.pretrain)\n",
        "    net.load_state_dict(weights,strict=False)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=800, gamma=0.1) #learning rate decay\n",
        "\n",
        "\n",
        "'''load pretrained model'''\n",
        "if args.weights != 0:\n",
        "    print(f'=> resuming from {args.weights}')\n",
        "    assert os.path.exists(args.weights)\n",
        "    checkpoint_file = os.path.join(args.weights)\n",
        "    assert os.path.exists(checkpoint_file)\n",
        "    loc = 'cuda:{}'.format(args.gpu_device)\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=loc)\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    best_tol = checkpoint['best_tol']\n",
        "    net.load_state_dict(checkpoint['state_dict'],strict=False)\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    args.path_helper = checkpoint['path_helper']\n",
        "    logger = create_logger(args.path_helper['log_path'])\n",
        "    print(f'=> loaded checkpoint {checkpoint_file} (epoch {start_epoch})')\n",
        "\n",
        "args.path_helper = set_log_dir('logs', args.exp_name) # set_log_dir from utils.py\n",
        "logger = create_logger(args.path_helper['log_path'])\n",
        "logger.info(args)\n",
        "\n",
        "\n",
        "'''segmentation data'''\n",
        "\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "\n",
        "        ]\n",
        "    )\n",
        "\n",
        "if 'brats' in args.dataset: # BraTS dataset\n",
        "    '''Brain Tumor data'''\n",
        "    brats_train_dataset = Brats(args, args.data_path, mode = 'Training' , transform = train_transforms)\n",
        "    brats_test_dataset = Brats(args, args.data_path, mode = 'Validation' , transform =  train_transforms)\n",
        "\n",
        "    dataset_size = len(brats_train_dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(0.2 * dataset_size))\n",
        "    np.random.shuffle(indices)\n",
        "    train_sampler = SubsetRandomSampler(indices[split:])\n",
        "    test_sampler = SubsetRandomSampler(indices[:split])\n",
        "\n",
        "    nice_train_loader = DataLoader(brats_train_dataset, batch_size=args.b, sampler = train_sampler,  num_workers=6, pin_memory=True)\n",
        "    nice_test_loader = DataLoader(brats_test_dataset, batch_size=args.b, sampler=test_sampler,  num_workers=6, pin_memory=True)\n",
        "\n",
        "    '''end'''\n",
        "\n",
        "\n",
        "'''checkpoint path and tensorboard'''\n",
        "checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, settings.TIME_NOW)\n",
        "#use tensorboard\n",
        "if not os.path.exists(settings.LOG_DIR):\n",
        "    os.mkdir(settings.LOG_DIR)\n",
        "writer = tensorboard.writer.SummaryWriter(log_dir=os.path.join(\n",
        "        settings.LOG_DIR, args.net, args.path_helper['log_path'].split('/')[1]))\n",
        "#create checkpoint folder to save model\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
        "\n",
        "'''begain training'''\n",
        "best_acc = 0.0\n",
        "best_tol = 1e4\n",
        "best_dice = 0.0\n",
        "best_loss = 10000.0\n",
        "\n",
        "for epoch in range(settings.EPOCH):\n",
        "    net.train()\n",
        "    time_start = time.time()\n",
        "    loss, current_lr = function.train_sam(args, net, optimizer, nice_train_loader, epoch, writer, vis = args.vis)\n",
        "    logger.info(f'Train loss: {loss} || @ epoch {epoch} || @ lr {current_lr}.')\n",
        "    writer.add_scalar(\"Train_Loss\", loss, epoch)\n",
        "\n",
        "    if loss < best_loss:\n",
        "        print('SAVING CHECKPOINT! - BEST LOSS')\n",
        "        best_loss = loss\n",
        "        is_best = True\n",
        "\n",
        "        save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'model': args.net,\n",
        "        'state_dict': net.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "       # 'best_tol': tol,\n",
        "        'path_helper': args.path_helper,\n",
        "    }, is_best, args.path_helper['ckpt_path'], filename=\"best_loss\")\n",
        "    else:\n",
        "        is_best = False\n",
        "\n",
        "\n",
        "    net.eval()\n",
        "    if epoch:\n",
        "        tol, (eiou, edice) = function.validation_sam(args, nice_test_loader, epoch, net, writer)\n",
        "        logger.info(f'Total score: {tol}, IOU: {eiou}, DICE: {edice} || @ epoch {epoch}.')\n",
        "        writer.add_scalar(\"Dice_Valid\", edice, epoch)\n",
        "\n",
        "        if args.distributed != 'none':\n",
        "            sd = net.module.state_dict()\n",
        "        else:\n",
        "            sd = net.state_dict()\n",
        "\n",
        "        if edice > best_dice:\n",
        "            print('SAVING CHECKPOINT! - BEST DICE')\n",
        "            best_dice = edice\n",
        "            is_best = True\n",
        "\n",
        "            save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'model': args.net,\n",
        "            'state_dict': sd,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'best_tol': tol,\n",
        "            'path_helper': args.path_helper,\n",
        "        }, is_best, args.path_helper['ckpt_path'], filename=\"best_dice\")\n",
        "        else:\n",
        "            is_best = False\n",
        "\n",
        "writer.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}